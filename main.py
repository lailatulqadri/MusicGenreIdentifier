# -*- coding: utf-8 -*-
"""TA2023 Text Analytic Applications: Document Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pd0VSYNfmh_YZUoLIMEPGasvn6_zm9kC

We will use nltk toolkit to assist text cleaning.
"""

#To import and download nltk
import nltk
nltk.download('stopwords')
nltk.download('wordnet')

"""We use sklearn library for machine learning based analysis. Sklearn support many machine learning approaches such as SVM, nearest neighbour, random forest, k-means and many more. You can find more information about sklearn in this webpage: https://scikit-learn.org/stable/"""

# Importing libraries
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cluster import KMeans
from nltk.corpus import stopwords
from nltk.stem.wordnet import WordNetLemmatizer
import string
import re
import numpy as np
from collections import Counter

stop = set(stopwords.words('english'))
exclude = set(string.punctuation)
lemma = WordNetLemmatizer()

"""Yup, when you work with text, text cleaning is a crucial task! This simple function allows you to clean doc. You can modify this function to fit your text cleaning processes."""

# Cleaning the text sentences so that punctuation marks, stop words & digits are removed
def clean(doc):
    stop_free = " ".join([i for i in doc.lower().split() if i not in stop])
    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)
    normalized = " ".join(lemma.lemmatize(word) for word in punc_free.split())
    processed = re.sub(r"\d+","",normalized)
    y = processed.split()
    return y

"""Document Representations used in this example is Tf-Idf. You already learn this method last 2 weeks.

---
Term frequency and inverse document frequency (tf-idf) is a word weighing scheme. The tf-idf value increases as the number of times a word appears in a document. So, it helps to reflect the importance of a word in a corpus.

"""

import pandas as pd

"""Describing the information/categories in the dataset. The dataset is not labeled."""

print ("There are 10 sentences of following three classes on which K-NN classification and K-means clustering"\
         " is performed : \n1. Cricket \n2. Artificial Intelligence \n3. Chemistry")

"""Let's read our data!"""

# read traning dataset
dataset = pd.read_csv('Clustering dataset.csv')
print("Data\n",dataset)

train_clean_sentences = []

"""Text Preprocessing - for each document in our dataset, we will give it to clean function above. The document can be found in dataset['Text'] <- inside column 'Text'"""

for line in dataset["Text"]:
    line = line.strip()
    cleaned = clean(line)
    cleaned = ' '.join(cleaned)
    train_clean_sentences.append(cleaned)

"""Output after text preprocessing"""

print(train_clean_sentences)

"""Text representation by using TFIDF"""

vectorizer = TfidfVectorizer(stop_words='english')
X = vectorizer.fit_transform(train_clean_sentences)

"""Let's split the data into training our model. For easy understanding, we will label our dataset to 0, 1 & 2."""

# Creating true labels for 30 training sentences
y_train = np.zeros(30)  # label 0
y_train[10:20] = 1      # label 1
y_train[20:30] = 2      # label 2

"""# 3. Training the Classification (K-NN) & Clustering (K-Means) models.

---
Output for k-NN classification is category membership. Texts are assigned to class that is most common among their k nearest neighbours.

Clustering determines the grouping of a set of data.In K-Means clustering, the 'K' cluster centers are the centroid of data points of a cluster.

"""

# Classification the document with KNN classifier
modelknn = KNeighborsClassifier(n_neighbors=5)
modelknn.fit(X,y_train)

# Clustering the training 30 sentences with K-means technique
modelkmeans = KMeans(n_clusters=3, init='k-means++', max_iter=200, n_init=100)
modelkmeans.fit(X)

"""Testing on Unseen Texts.

---
This is to test the trained model. It is tested using 3 text sentences.

"""

# Predicting it on test data : Testing Phase
test_sentences = ["Chemical compunds are used for preparing bombs based on some reactions",\
"Cricket is a boring game where the batsman only enjoys the game",\
"Machine learning is a area of Artificial intelligence"]

test_clean_sentence = []
for test in test_sentences:
  cleaned_test = clean(test)
  cleaned = ' '.join(cleaned_test)
  cleaned = re.sub(r"\d+","",cleaned)
  test_clean_sentence.append(cleaned)

Test = vectorizer.transform(test_clean_sentence)

true_test_labels = ['Cricket','AI','Chemistry']
#classification
predicted_labels_knn = modelknn.predict(Test)
#clustering
predicted_labels_kmeans = modelkmeans.predict(Test)

print ("\nBelow 3 sentences will be predicted against the learned nieghbourhood and learned clusters:\n1. ",\
test_sentences[0],"\n2. ",test_sentences[1],"\n3. ",test_sentences[2])
print ("\n-------------------------------PREDICTIONS BY KNN------------------------------------------")
print ("\n",test_sentences[0],":",true_test_labels[np.int(predicted_labels_knn[0])],\
"\n",test_sentences[1],":",true_test_labels[np.int(predicted_labels_knn[1])],\
"\n",test_sentences[2],":",true_test_labels[np.int(predicted_labels_knn[2])])

print ("\n-------------------------------PREDICTIONS BY K-Means--------------------------------------")
print ("\nIndex of Cricket cluster : ",Counter(modelkmeans.labels_[0:10]).most_common(1)[0][0])
print ("Index of Artificial Intelligence cluster : ",Counter(modelkmeans.labels_[10:20]).most_common(1)[0][0])
print ("Index of Chemistry cluster : ",Counter(modelkmeans.labels_[20:30]).most_common(1)[0][0])

print ("\n",test_sentences[0],":",predicted_labels_kmeans[0],\
"\n",test_sentences[1],":",predicted_labels_kmeans[1],\
"\n",test_sentences[2],":",predicted_labels_kmeans[2])

"""From the output above, we can see that all of the 3 sentences tested are correctly clustered and classified into categories."""
